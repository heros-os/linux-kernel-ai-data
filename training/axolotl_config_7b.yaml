# Training Configuration for smaller 7B/9B models
# For GPUs with 24GB VRAM (RTX 3090/4090)

base_model: deepseek-ai/deepseek-coder-6.7b-instruct
model_type: AutoModelForCausalLM
tokenizer_type: AutoTokenizer
trust_remote_code: true

# QLoRA 4-bit quantization
load_in_4bit: true
adapter: qlora
lora_r: 32
lora_alpha: 16
lora_dropout: 0.05
lora_target_linear: true

# Alternative base models (uncomment one):
# base_model: Qwen/Qwen2.5-Coder-7B-Instruct
# base_model: codellama/CodeLlama-7b-Instruct-hf
# base_model: mistralai/Mistral-7B-Instruct-v0.3

# Dataset
datasets:
  - path: ./exports/training_data.jsonl
    type: alpaca
    ds_type: json

# Smaller sequence length for 24GB VRAM
sequence_len: 2048
sample_packing: true
pad_to_sequence_len: true

micro_batch_size: 2
gradient_accumulation_steps: 4
num_epochs: 3
learning_rate: 2e-4
lr_scheduler: cosine
warmup_ratio: 0.03

optimizer: adamw_torch_fused
bf16: true
tf32: true
gradient_checkpointing: true
flash_attention: true

output_dir: ./models/kernel-coder-7b-lora
logging_steps: 10
save_steps: 500
