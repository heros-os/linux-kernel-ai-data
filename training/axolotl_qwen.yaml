# Training Configuration for Axolotl
# Fine-tune Qwen 2.5 Coder 7B on XBMC commit data
# Works on RTX 3090/4090 with 24GB VRAM

base_model: Qwen/Qwen2.5-Coder-7B-Instruct
model_type: AutoModelForCausalLM
tokenizer_type: AutoTokenizer
trust_remote_code: true

# Load in 4-bit for reduced VRAM usage (QLoRA)
load_in_4bit: true
adapter: qlora
lora_r: 32
lora_alpha: 16
lora_dropout: 0.05
lora_target_linear: true

# Dataset configuration - XBMC training data
datasets:
  - path: ./exports/xbmc_training.jsonl
    type: alpaca
    ds_type: json

# Training parameters
sequence_len: 2048
sample_packing: true
pad_to_sequence_len: true

micro_batch_size: 2
gradient_accumulation_steps: 4
num_epochs: 1
learning_rate: 2e-4
lr_scheduler: cosine
warmup_ratio: 0.03
weight_decay: 0.01

optimizer: adamw_torch_fused
bf16: true
tf32: true
gradient_checkpointing: true
flash_attention: true

# Output
output_dir: ./models/xbmc-coder-qwen-lora
logging_steps: 10
save_steps: 500
eval_steps: 500
