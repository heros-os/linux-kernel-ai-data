{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üöÄ Fine-Tune Qwen2.5-Coder for Linux Kernel Code Review\n",
        "\n",
        "This notebook fine-tunes a code reviewer model using your Linux kernel training data.\n",
        "\n",
        "## Steps:\n",
        "1. Install libraries\n",
        "2. Mount Google Drive (where your data is)\n",
        "3. Train the model\n",
        "4. Export to GGUF format for Ollama\n",
        "5. Download the model\n",
        "\n",
        "**Make sure to select GPU runtime**: Runtime ‚Üí Change runtime type ‚Üí T4 GPU"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install Unsloth and dependencies\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\" -q\n",
        "!pip install transformers datasets trl accelerate peft bitsandbytes -q\n",
        "print(\"‚úÖ Libraries installed!\")"
      ],
      "metadata": {
        "id": "install"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"‚úÖ Google Drive mounted!\")"
      ],
      "metadata": {
        "id": "mount_drive"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Verify your data file exists\n",
        "import os\n",
        "\n",
        "# UPDATE THIS PATH if your file is in a different location\n",
        "DATA_FILE = \"/content/drive/MyDrive/premium_reasoning.jsonl\"\n",
        "\n",
        "if os.path.exists(DATA_FILE):\n",
        "    size_mb = os.path.getsize(DATA_FILE) / 1_000_000\n",
        "    print(f\"‚úÖ Found data file: {DATA_FILE}\")\n",
        "    print(f\"   Size: {size_mb:.2f} MB\")\n",
        "else:\n",
        "    print(f\"‚ùå File not found: {DATA_FILE}\")\n",
        "    print(\"   Please upload premium_reasoning.jsonl to your Google Drive root folder\")\n",
        "    print(\"   Or update the DATA_FILE path above\")"
      ],
      "metadata": {
        "id": "verify_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Load the model\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "print(\"Loading Qwen2.5-Coder-7B...\")\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/Qwen2.5-Coder-7B\",\n",
        "    max_seq_length=4096,\n",
        "    load_in_4bit=True,\n",
        "    dtype=None,\n",
        ")\n",
        "print(\"‚úÖ Model loaded!\")"
      ],
      "metadata": {
        "id": "load_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Apply LoRA for efficient fine-tuning\n",
        "print(\"Applying LoRA adapters...\")\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        ")\n",
        "print(\"‚úÖ LoRA applied!\")"
      ],
      "metadata": {
        "id": "apply_lora"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Load and format training data\n",
        "from datasets import load_dataset\n",
        "\n",
        "print(\"Loading dataset...\")\n",
        "dataset = load_dataset(\"json\", data_files=DATA_FILE)\n",
        "\n",
        "def format_instruction(example):\n",
        "    \"\"\"Format data for instruction tuning.\"\"\"\n",
        "    return {\n",
        "        \"text\": f\"\"\"### Instruction\n",
        "{example['instruction']}\n",
        "\n",
        "### Input\n",
        "{example['input']}\n",
        "\n",
        "### Response\n",
        "{example['output']}\"\"\"\n",
        "    }\n",
        "\n",
        "dataset = dataset[\"train\"].map(format_instruction)\n",
        "print(f\"‚úÖ Dataset loaded: {len(dataset)} examples\")"
      ],
      "metadata": {
        "id": "load_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Configure training\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=4096,\n",
        "    args=TrainingArguments(\n",
        "        output_dir=\"/content/kernel_reviewer\",\n",
        "        num_train_epochs=1,  # Increase for better results\n",
        "        per_device_train_batch_size=1,\n",
        "        gradient_accumulation_steps=8,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=True,\n",
        "        logging_steps=10,\n",
        "        save_steps=500,\n",
        "        warmup_steps=50,\n",
        "        optim=\"adamw_8bit\",\n",
        "        seed=42,\n",
        "    ),\n",
        ")\n",
        "print(\"‚úÖ Trainer configured!\")"
      ],
      "metadata": {
        "id": "configure_training"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: TRAIN! üöÄ\n",
        "print(\"=\"*50)\n",
        "print(\"üöÄ STARTING TRAINING\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Dataset size: {len(dataset)} examples\")\n",
        "print(f\"Epochs: 1\")\n",
        "print(f\"Estimated time: ~30-60 minutes on T4 GPU\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "trainer_stats = trainer.train()\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"‚úÖ TRAINING COMPLETE!\")\n",
        "print(\"=\"*50)"
      ],
      "metadata": {
        "id": "train"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9: Save the fine-tuned model\n",
        "print(\"Saving model...\")\n",
        "model.save_pretrained(\"/content/kernel_reviewer\")\n",
        "tokenizer.save_pretrained(\"/content/kernel_reviewer\")\n",
        "print(\"‚úÖ Model saved!\")"
      ],
      "metadata": {
        "id": "save_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 10: Export to GGUF format for Ollama\n",
        "print(\"Exporting to GGUF format (for Ollama)...\")\n",
        "print(\"This may take 10-15 minutes...\")\n",
        "\n",
        "model.save_pretrained_gguf(\n",
        "    \"/content/kernel_reviewer_gguf\",\n",
        "    tokenizer,\n",
        "    quantization_method=\"q4_k_m\"  # Good balance of quality/size\n",
        ")\n",
        "\n",
        "print(\"‚úÖ GGUF export complete!\")"
      ],
      "metadata": {
        "id": "export_gguf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 11: Copy to Google Drive for safekeeping\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# Create output directory in Drive\n",
        "output_dir = \"/content/drive/MyDrive/kernel_reviewer_model\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Copy GGUF file\n",
        "gguf_files = [f for f in os.listdir(\"/content/kernel_reviewer_gguf\") if f.endswith('.gguf')]\n",
        "for f in gguf_files:\n",
        "    src = f\"/content/kernel_reviewer_gguf/{f}\"\n",
        "    dst = f\"{output_dir}/{f}\"\n",
        "    print(f\"Copying {f} to Google Drive...\")\n",
        "    shutil.copy(src, dst)\n",
        "\n",
        "print(f\"\\n‚úÖ Model saved to Google Drive: {output_dir}\")"
      ],
      "metadata": {
        "id": "copy_to_drive"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 12: Download GGUF file to your computer\n",
        "from google.colab import files\n",
        "\n",
        "print(\"Downloading GGUF file to your computer...\")\n",
        "print(\"(This may take a few minutes depending on file size)\")\n",
        "\n",
        "# Find the GGUF file\n",
        "gguf_path = \"/content/kernel_reviewer_gguf\"\n",
        "gguf_files = [f for f in os.listdir(gguf_path) if f.endswith('.gguf')]\n",
        "\n",
        "if gguf_files:\n",
        "    gguf_file = os.path.join(gguf_path, gguf_files[0])\n",
        "    files.download(gguf_file)\n",
        "    print(f\"\\n‚úÖ Downloaded: {gguf_files[0]}\")\n",
        "else:\n",
        "    print(\"‚ùå No GGUF file found\")"
      ],
      "metadata": {
        "id": "download"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üéâ Done!\n",
        "\n",
        "## Next Steps:\n",
        "\n",
        "1. **Save the downloaded GGUF file** to a folder on your computer\n",
        "\n",
        "2. **Create a Modelfile** in the same folder:\n",
        "```\n",
        "FROM ./unsloth.Q4_K_M.gguf\n",
        "\n",
        "SYSTEM \"You are a Linux kernel developer. Given a commit message describing a bug fix, feature, or improvement, and the relevant source code context, generate the appropriate kernel patch.\"\n",
        "\n",
        "PARAMETER temperature 0.3\n",
        "PARAMETER top_p 0.9\n",
        "```\n",
        "\n",
        "3. **Import into Ollama**:\n",
        "```bash\n",
        "cd /path/to/your/model/folder\n",
        "ollama create kernel-reviewer -f Modelfile\n",
        "```\n",
        "\n",
        "4. **Test your model**:\n",
        "```bash\n",
        "ollama run kernel-reviewer\n",
        "```"
      ],
      "metadata": {
        "id": "next_steps"
      }
    }
  ]
}
